FROM jupyter/scipy-notebook

USER root

# Versions
ARG ARC_JUPYTER_VERSION
ARG SPARK_VERSION
ARG HADOOP_VERSION

# test to ensure all arguments have values then set environment variable
RUN test -n "$ARC_JUPYTER_VERSION"
ENV ARC_JUPYTER_VERSION $ARC_JUPYTER_VERSION
RUN test -n "$SPARK_VERSION"
ENV SPARK_VERSION $SPARK_VERSION
RUN test -n "$HADOOP_VERSION"
ENV HADOOP_VERSION $HADOOP_VERSION

ENV SCALA_VERSION         2.11
ENV SPARK_BASE            /usr/local
ENV SPARK_JARS            /usr/local/jars
ENV HADOOP_HOME           /opt/hadoop
ENV SPARK_DOWNLOAD_URL    https://www-us.apache.org/dist/spark
ENV SPARK_CHECKSUM_URL    https://archive.apache.org/dist/spark
ENV HADOOP_DOWNLOAD_URL   https://www-us.apache.org/dist/hadoop/common
ENV HADOOP_CHECKSUM_URL   https://www.apache.org/dist/hadoop/common
ENV JAVA_OPTS             "-Xmx1g"
ENV BASE_JAVA_OPTS        "-XX:+UseG1GC -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap"
ENV SPARK_HOME            /usr/local/spark
ENV PYTHONPATH            $SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.7-src.zip
ENV SPARK_OPTS            --driver-java-options=-Xms1024M --driver-java-options=-Xmx4096M --driver-java-options=-Dlog4j.logLevel=info

RUN apt-get update && \
  apt-get install --no-install-recommends -y openjdk-8-jre-headless ca-certificates-java wget && \
  rm -rf /var/lib/apt/lists/*

# add spark-without-hadoop
RUN mkdir -p ${SPARK_BASE} && \
  wget -O spark.sha ${SPARK_CHECKSUM_URL}/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-without-hadoop.tgz.sha512 && \
  export SPARK_SHA512_SUM=$(grep -o "[A-F0-9]\{8\}" spark.sha | awk '{print}' ORS='' | tr '[:upper:]' '[:lower:]') && \
  rm -f spark.sha && \
  wget -O spark.tar.gz ${SPARK_DOWNLOAD_URL}/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-without-hadoop.tgz && \
  echo "${SPARK_SHA512_SUM}  spark.tar.gz" | sha512sum -c - && \
  gunzip -c spark.tar.gz | tar -xf - -C $SPARK_BASE --strip-components=1 && \
  rm -f spark.tar.gz

# add hadoop
RUN mkdir -p ${HADOOP_HOME} && \
  wget -O hadoop.mds ${HADOOP_CHECKSUM_URL}/hadoop-${HADOOP_VERSION}/hadoop-${HADOOP_VERSION}.tar.gz.mds && \
  export HADOOP_SHA512_SUM=$(cat hadoop.mds | grep SHA512 -A1 | cut -d'=' -f2 | tr -d '\n' | tr -d ' ' | tr '[:upper:]' '[:lower:]') && \
  rm -f hadoop.mds && \
  wget -O hadoop.tar.gz ${HADOOP_DOWNLOAD_URL}/hadoop-${HADOOP_VERSION}/hadoop-${HADOOP_VERSION}.tar.gz && \
  echo "${HADOOP_SHA512_SUM}  hadoop.tar.gz" | sha512sum -c - && \
  gunzip -c hadoop.tar.gz | tar -xf - -C $HADOOP_HOME --strip-components=1 && \
  rm -f hadoop.tar.gz

# link hadoop to spark as per https://spark.apache.org/docs/latest/hadoop-provided.html
RUN echo "export SPARK_DIST_CLASSPATH=\$(/opt/hadoop/bin/hadoop classpath)" >> ${SPARK_BASE}/conf/spark-env.sh

# create symlink for Jupyter
RUN cd /usr/local && ln -s spark-${APACHE_SPARK_VERSION}-bin-hadoop${HADOOP_VERSION} spark

# download library and dependencies
USER $NB_UID
RUN cd /tmp && \
  wget -P /tmp https://git.io/coursier-cli && \
  chmod +x /tmp/coursier-cli && \
  /tmp/coursier-cli bootstrap \
  --embed-files=false \
  --force-version com.fasterxml.jackson.core:jackson-databind:2.6.7.1 \
  --force-version org.json4s:json4s-ast_${SCALA_VERSION}:3.5.3 \
  --force-version org.json4s:json4s-core_${SCALA_VERSION}:3.5.3 \
  --force-version org.json4s:json4s-jackson_${SCALA_VERSION}:3.5.3 \
  --force-version org.json4s:json4s-scalap_${SCALA_VERSION}:3.5.3 \
  --force-version com.google.guava:guava:14.0.1 \
  --force-version org.slf4j:slf4j-log4j12:1.7.16 \
  --exclude org.slf4j:slf4j-nop \
  ai.tripl:arc-jupyter_${SCALA_VERSION}:${ARC_JUPYTER_VERSION} \
  ai.tripl:arc-cassandra-pipeline-plugin_${SCALA_VERSION}:1.0.1 \
  ai.tripl:arc-deltaperiod-config-plugin_${SCALA_VERSION}:1.0.0 \
  ai.tripl:arc-deltalake-pipeline-plugin_${SCALA_VERSION}:1.0.0 \
  ai.tripl:arc-elasticsearch-pipeline-plugin_${SCALA_VERSION}:1.0.0 \
  ai.tripl:arc-kafka-pipeline-plugin_${SCALA_VERSION}:1.0.0 \
  ai.tripl:arc-mongodb-pipeline-plugin_${SCALA_VERSION}:1.0.0 \
  com.facebook.presto:presto-jdbc:0.209 \
  com.microsoft.sqlserver:mssql-jdbc:7.2.1.jre8 \
  mysql:mysql-connector-java:5.1.47 \
  org.apache.hadoop:hadoop-aws:${HADOOP_VERSION} \
  org.apache.hadoop:hadoop-azure:${HADOOP_VERSION} \
  org.postgresql:postgresql:42.2.6 \
  com.amazon.redshift:redshift-jdbc42:1.2.27.1051 \
  -o arc && \
  ./arc --install --force && \
  rm /tmp/arc && \
  rm /tmp/coursier-cli

# add Download as: Arc (.json) to jupyter notebook
RUN pip install git+https://github.com/tripl-ai/nb_extension_arcexport.git

# override default run command to allow JAVA_OPTS injection
COPY arc-jupyter/kernel.json /home/jovyan/.local/share/jupyter/kernels/arc/kernel.json

# Install pyarrow
RUN conda install --quiet -y 'pyarrow' && \
  conda clean --all -f -y && \
  fix-permissions $CONDA_DIR && \
  fix-permissions /home/$NB_USER

# add the execution time extension
RUN pip install jupyter_contrib_nbextensions && \
  jupyter contrib nbextension install --user && \
  jupyter nbextensions_configurator disable --user && \
  jupyter nbextension enable execute_time/ExecuteTime

# ui tweaks
# css to maximise screen realestate
COPY arc-jupyter/custom.css /home/jovyan/.jupyter/custom/custom.css
# js to set code formatting for arc commmands
COPY arc-jupyter/custom.js /home/jovyan/.jupyter/custom/custom.js
# copy in settings to fix tabsize
COPY arc-jupyter/notebook.json /home/jovyan/.jupyter/nbconfig

# disable default save data with notebooks
COPY arc-jupyter/scrub_output_pre_save.py /tmp/scrub_output_pre_save.py
RUN cat /tmp/scrub_output_pre_save.py >> /etc/jupyter/jupyter_notebook_config.py

# set permissions
USER root
RUN chown -R jovyan:users /home/jovyan
USER $NB_UID
